{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictors of Non-Voters in the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('no_voter_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessary columns for race \n",
    "df['white'] = np.where(df['race'] == 'White', 1, 0)\n",
    "df['black'] = np.where(df['race'] == 'Black', 1, 0)\n",
    "df['other/mixed'] = np.where(df['race'] == 'Other/Mixed', 1, 0)\n",
    "df['hispanic'] = np.where(df['race'] == 'Hispanic', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender \n",
    "df['gender'] = np.where(df['gender'] == 'Female', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income category\n",
    "new_label = {\"income_cat\": {\"Less than $40k\":1, \n",
    "            \"$40-75k\":2, \n",
    "            \"$75-125k\":3,\n",
    "            \"$125k or more\":4}}\n",
    "df.replace(new_label, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education category\n",
    "new_label = {\"educ\": {\"High school or less\":1, \n",
    "            \"Some college\":2, \n",
    "            \"College\":3}}\n",
    "df.replace(new_label, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voter category data manipulations\n",
    "new_label = {\"voter_category\": {\"always\":3, \"sporadic\":2, \"rarely/never\":1}}\n",
    "df.replace(new_label, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPP\n",
    "df = df.drop([\"race\", \"RespId\", \"weight\"], axis=1)\n",
    "# fill NA's \n",
    "df_2 = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_2[\"voter_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.drop(\"voter_category\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD\n",
    "# use different # of cols and find which # minimizes error\n",
    "def truncated_svd(X, y):\n",
    "    svd_errors = []\n",
    "    w_hats_svd = []\n",
    "    _, p = X.shape\n",
    "    u, s, vt = la.svd(X)\n",
    "    sigma = np.zeros(X.shape)\n",
    "    sigma1 = sigma.copy()\n",
    "\n",
    "    for i in range(p):\n",
    "        s1 = s[:i+1]\n",
    "        np.fill_diagonal(sigma, np.append(s1, np.zeros(p - i + 1)))\n",
    "        sigma_inv = la.pinv(sigma)\n",
    "        w_hat_svd = vt.T@sigma_inv@u.T@y\n",
    "        w_hats_svd.append(w_hat_svd)\n",
    "        y_hat_svd = X@w_hat_svd\n",
    "        svd_errors.append(np.mean(y - y_hat_svd)**2)\n",
    "\n",
    "    best_param_svd = svd_errors.index(min(svd_errors))\n",
    "    return best_param_svd, w_hats_svd[best_param_svd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_best_num_col, svd_whats = truncated_svd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "def rls(X, y, lambda_vals):\n",
    "    \n",
    "    #### RLS ####\n",
    "    w_hat_lst = []\n",
    "    errors_lst = []\n",
    "    u, s, vt = la.svd(X)\n",
    "    sigma = np.zeros(X.shape) # n x p\n",
    "    sigma1 = sigma.copy() #creating copy \n",
    "    np.fill_diagonal(sigma1, s)\n",
    "            \n",
    "    for l, val in enumerate(lambda_vals):\n",
    "        w_hat = la.inv(vt.T@sigma1.T@sigma1@vt + val)@vt.T@sigma1.T@u.T@y\n",
    "        w_hat_lst.append(w_hat)\n",
    "\n",
    "        y_tilde = X@w_hat\n",
    "        # error = la.norm(y - y_tilde)\n",
    "        error = np.mean((y_tilde - y)**2)\n",
    "        errors_lst.append(error)\n",
    "\n",
    "    best_param_index = errors_lst.index(min(errors_lst))\n",
    "    best_w = w_hat_lst[best_param_index]\n",
    "    best_lambda = lambda_vals[best_param_index]\n",
    "    return best_lambda, best_w \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vals = np.array([0, 0.001, 0.01, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64])\n",
    "\n",
    "rls_best_lambda, rls_whats = rls(X, y, lambda_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PCA\n",
    "u,s,vh = np.linalg.svd(X_train ,full_matrices=False)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(range(1,s.shape[0]+1), s, color=\"steelblue\", label='sigmas')\n",
    "ax.set_xlabel(\"sigma_index\", fontsize=16)\n",
    "ax.set_ylabel(\"value\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# first 3 sigmas seem to be the most important; we will solve for 3 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error distance: 50.142760706404495\n"
     ]
    }
   ],
   "source": [
    "# calculate principal components\n",
    "pc = X_train@vh[:3,:].T\n",
    "z3 = pc\n",
    "\n",
    "# use 3 PC to compute least sqaures \n",
    "w = np.linalg.inv(z3.T@z3)@z3.T@y_train\n",
    "y_pred = z3@w\n",
    "acc_train = np.mean((y_pred - y_train)**2)\n",
    "acc_train = la.norm(y_pred - y_train)\n",
    "print(\"training error distance:\", acc_train)\n",
    "\n",
    "# What to make of this? \n",
    "# Watch a video? lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# sigma_sq is our parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernal Ridge\n",
    "def kernel_ridge(X, y, sigma, lam):\n",
    "    n, p = X.shape\n",
    "    norms2 = (np.array(la.norm(X, axis =1)).T)**2 # squared norm of each training sample\n",
    "    innerProds = X@X.T\n",
    "\n",
    "    # squared distances between each pair of training samples\n",
    "    dist2 = np.matrix(norms2).T@np.ones([1, n]) + np.ones([n, 1])@np.matrix(norms2) - 2*innerProds \n",
    "    K = np.exp(-dist2/(2*sigma))\n",
    "    alpha = (la.inv(K + lam*np.identity(n))@y)\n",
    "    yhat = K@alpha.T\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial sigma/lam\n",
    "sigma = .05\n",
    "lam = 1\n",
    "kr_alpha = kernel_ridge(X_train, y_train, sigma, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4668,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4668 is the # of training samples; alpha is in R^n \n",
    "kr_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(score, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Sigmoid function with a threshold\n",
    "    :param score: (float) A real valued number to convert into a number between 0 and 1\n",
    "    :param threshold: (float) Prevent overflow of exp by capping activation at 20.\n",
    "    \n",
    "    :return: (float) sigmoid function result.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function to restrict the value of the input score and return the output of applying the \n",
    "    #       sigmoid function to it (Please do not use external libraries)\n",
    "    \n",
    "    if abs(score) > abs(threshold):\n",
    "        score = math.copysign(threshold,score)\n",
    "\n",
    "    sigmoid = 1 / (1 + math.exp(-score))\n",
    "    \n",
    "    return sigmoid\n",
    "\n",
    "    \n",
    "def sigmoid_grad(y, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Derivative/gradient of the sigmoid function.\n",
    "    :param y: (float) A real valued input for which to compute the derivative.\n",
    "    :param threshold: (float) Prevent overflow of exp by capping activation at 20.\n",
    "    \n",
    "    :return: (float) sigmoid derivative function result.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function to return the output of applying the gradient of the sigmoid\n",
    "    # function to the input score (Please do not use external libraries)\n",
    "\n",
    "    if abs(y) > abs(threshold):\n",
    "        y = math.copysign(threshold,y)\n",
    "    deriv = (sigmoid(y)**2)*math.exp(-y)\n",
    "    \n",
    "    return deriv\n",
    "    \n",
    "def shuffle(X, y):\n",
    "    \"\"\" Shuffle training data \"\"\"\n",
    "    shuffled_indices = np.random.permutation(len(y))\n",
    "    #print(\"?!\", shuffled_indices)\n",
    "    return X.iloc[shuffled_indices], y.iloc[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, num_features, eta):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: (int) The number of features (including bias)\n",
    "        :param eta: (float) learning rate\n",
    "        \"\"\"\n",
    "        self.w = np.zeros((num_features,1))\n",
    "        self.eta = eta\n",
    "\n",
    "    def progress(self, examples_x, examples_y):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the probability and accuracy\n",
    "        :param examples_x: (2D np.ndarray) The features from the dataset to score\n",
    "        :param examples_y: (1D np.ndarray) The labels from the dataset to score\n",
    "\n",
    "        :return: (float, float) A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        logprob = 0.0\n",
    "        num_right = 0\n",
    "        for x_i, y in zip(examples_x, examples_y):\n",
    "            p = sigmoid(self.w.dot(x_i))\n",
    "            if y == 1:\n",
    "                logprob += math.log(p)\n",
    "            else:\n",
    "                logprob += math.log(1.0 - p)\n",
    "\n",
    "            # Get accuracy\n",
    "            if abs(y - p) <= 0.5:\n",
    "                num_right += 1\n",
    "\n",
    "        return logprob, float(num_right) / float(len(examples_y))\n",
    "\n",
    "    def sgd_update(self, x_i, y, lam=0.0):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param x_i: (1D np.ndarray) The features of the example to take the gradient with respect to\n",
    "        :param y: (float) The target output of the example to take the gradient with respect to\n",
    "        :param lam: (float) regularization term. Default is zero; only used in Part 2D.\n",
    "        \n",
    "        :return: (1D np.ndarray) Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "\n",
    "        # compute pi\n",
    "        print(\"xi\", x_i)\n",
    "        pi = sigmoid(self.w.dot(x_i))\n",
    "        print(\"pi:\", pi)\n",
    "        \n",
    "        action = 2 * lam * self.w[1:]\n",
    "\n",
    "        # insert b_0\n",
    "        action = np.insert(action,0,0) \n",
    "        self.w = self.w + self.eta * ((y - pi) * x_i - action)\n",
    "        \n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5283    1\n",
       "Name: voter_category, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[[2559]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, eta, store_epoch, lam=1e-5, decay=0):\n",
    "    \"\"\"\n",
    "    Train a LogReg object for a set number of epochs with a given eta.\n",
    "    \n",
    "    :param epochs: (int) total number of training epochs\n",
    "    :param eta: (float) learning rate\n",
    "    :param store_epoch: (int) store training and test accuracies every store_epoch epochs\n",
    "    :param lam: (float) weight given to regularization term. Default 0. Only used in Part 2D. \n",
    "    :param decay: (float) Used to update learning rate during training (Part 3). \n",
    "                  Equals 0 when learning rate is constant throughout training (Part 2). \n",
    "                  \n",
    "    :return (train_accuracy_array, test_accuracy_array, learning_rates): tuple of (List, List, List)\n",
    "        :train_accuracy_array: training accuracy after every store_epoch epochs\n",
    "        :test_accuracy_array: test accuracy after every store_epoch epochs\n",
    "        :learning_rates: learning rate after every store_epoch epochs. All values in this list \n",
    "                         will be the same if decay = 0 (Only required for Part 2F)\n",
    "    \n",
    "        Example: With epochs = 30 and store_epoch = 10, only store accuracies after epochs = 10, 20, and 30.\n",
    "    \"\"\"\n",
    "    \n",
    "    lr = LogReg(X_train.shape[1], eta)\n",
    "\n",
    "    #assert dataset_handler.train_x.shape == (1105, 60) \n",
    "    #assert dataset_handler.test_x.shape == (277, 60) \n",
    "    \n",
    "    train_accuracy_array = []\n",
    "    test_accuracy_array = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "        # gradient descent update on each training example.\n",
    "\n",
    "        # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "        # to make sure the algorithm is truly learning properly on both training and test data\n",
    "        \n",
    "        # update learning rate\n",
    "        lr.eta = lr.eta / (1 + decay * epoch)\n",
    "        \n",
    "        # shuffle data\n",
    "        x, y = shuffle(X_train, y_train)\n",
    "        print(\"got here?\")\n",
    "        \n",
    "        # go through entire training data\n",
    "        for i, x_i in enumerate(x):\n",
    "            new_x = x[x_i].values.reshape((1,x.shape[0]))        \n",
    "            lr.sgd_update(new_x, y.iloc[i], lam)\n",
    "        \n",
    "        # store prorgress for each store_epoch\n",
    "        if epoch % store_epoch == 0:\n",
    "            _, train_accuracy = lr.progress(x, y)\n",
    "            train_accuracy_array.append(train_accuracy)\n",
    "            #_, test_accuracy = lr.progress(dataset_handler.test_x, dataset_handler.test_y)\n",
    "            # test_accuracy_array.append(test_accuracy)\n",
    "            learning_rates.append(lr.eta)\n",
    "            \n",
    "        \n",
    "    return train_accuracy_array, learning_rates\n",
    "    #test_accuracy_array,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here?\n",
      "xi [[1 1 1 ... 1 1 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m store_epoch \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_acc, _ \u001b[39m=\u001b[39m train(epochs, eta, store_epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_acc)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtrain accuracy after \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m epochs: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat((i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mstore_epoch, train_acc[i]))\n",
      "\u001b[1;32m/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb Cell 29\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, eta, store_epoch, lam, decay)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, x_i \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     new_x \u001b[39m=\u001b[39m x[x_i]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m,x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))        \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     lr\u001b[39m.\u001b[39;49msgd_update(new_x, y\u001b[39m.\u001b[39;49miloc[i], lam)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# store prorgress for each store_epoch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m store_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb Cell 29\u001b[0m in \u001b[0;36mLogReg.sgd_update\u001b[0;34m(self, x_i, y, lam)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# compute pi\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mxi\u001b[39m\u001b[39m\"\u001b[39m, x_i)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m pi \u001b[39m=\u001b[39m sigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw\u001b[39m.\u001b[39;49mdot(x_i))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpi:\u001b[39m\u001b[39m\"\u001b[39m, pi)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m lam \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;32m/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb Cell 29\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(score, threshold)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mSigmoid function with a threshold\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m:param score: (float) A real valued number to convert into a number between 0 and 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m:return: (float) sigmoid function result.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# TODO: Finish this function to restrict the value of the input score and return the output of applying the \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#       sigmoid function to it (Please do not use external libraries)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(score) \u001b[39m>\u001b[39m \u001b[39mabs\u001b[39m(threshold):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     score \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mcopysign(threshold,score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eujeneyum/Desktop/UChicago_Courses/Q4/ML/final_project/cmsc353_nonvoter_project/models.ipynb#X56sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sigmoid \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mscore))\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "eta  = 1e-4\n",
    "epochs = 300\n",
    "store_epoch = 50\n",
    "train_acc, _ = train(epochs, eta, store_epoch)\n",
    "\n",
    "for i in range(len(train_acc)):\n",
    "    print(\"\\ntrain accuracy after {} epochs: {}\".format((i+1)*store_epoch, train_acc[i]))\n",
    "    #print(\"test accuracy after {} epochs: {}\".format((i+1)*store_epoch, test_acc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?! [4235 3323 2930 ... 2750 1493 3967]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2_1</th>\n",
       "      <th>Q2_2</th>\n",
       "      <th>Q2_3</th>\n",
       "      <th>Q2_4</th>\n",
       "      <th>Q2_5</th>\n",
       "      <th>Q2_6</th>\n",
       "      <th>Q2_7</th>\n",
       "      <th>Q2_8</th>\n",
       "      <th>Q2_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Q32</th>\n",
       "      <th>Q33</th>\n",
       "      <th>ppage</th>\n",
       "      <th>educ</th>\n",
       "      <th>gender</th>\n",
       "      <th>income_cat</th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <th>other/mixed</th>\n",
       "      <th>hispanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4668 rows  119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Q1  Q2_1  Q2_2  Q2_3  Q2_4  Q2_5  Q2_6  Q2_7  Q2_8  Q2_9  ...  Q32  Q33  \\\n",
       "512    1     1     2     1     1     1     2     1     1     1  ...  1.0  0.0   \n",
       "4484   1     1     1     1     1     1     1     1     1     1  ...  0.0  0.0   \n",
       "1939   1     1     2     2     1     1     1     1     1     1  ...  0.0  0.0   \n",
       "1111   1     1     2     1     3     1     3     1     1     1  ...  1.0  0.0   \n",
       "4455   1     4     2     2     4     1     3     3     1     2  ...  0.0  2.0   \n",
       "...   ..   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
       "2007   1     1     4     3     3     1     2     1     1     4  ...  0.0  1.0   \n",
       "532    1     1     1     1     2     1     1     1     1     2  ...  0.0  0.0   \n",
       "303    1     1     1     2     2     1     1     2     1     4  ...  0.0  0.0   \n",
       "4832   1     1     1     1     1     1     1     1     2     2  ...  0.0  0.0   \n",
       "3925   1     1     1     1     3     1     3     1     1     1  ...  0.0  0.0   \n",
       "\n",
       "      ppage  educ  gender  income_cat  white  black  other/mixed  hispanic  \n",
       "512      73     3       0           2      0      1            0         0  \n",
       "4484     73     2       1           4      1      0            0         0  \n",
       "1939     29     2       1           2      1      0            0         0  \n",
       "1111     74     2       0           1      0      1            0         0  \n",
       "4455     37     3       0           3      0      1            0         0  \n",
       "...     ...   ...     ...         ...    ...    ...          ...       ...  \n",
       "2007     64     1       1           2      1      0            0         0  \n",
       "532      57     3       0           4      1      0            0         0  \n",
       "303      41     2       0           4      1      0            0         0  \n",
       "4832     45     1       0           3      1      0            0         0  \n",
       "3925     28     2       1           3      0      1            0         0  \n",
       "\n",
       "[4668 rows x 119 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = shuffle(X_train,y_train)\n",
    "X_train.iloc[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
